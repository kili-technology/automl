import os
from typing import List, Optional

import click
import numpy as np
import torch
from img2vec_pytorch import Img2Vec
from kili.client import Kili
from more_itertools import chunked
from numpy.testing import assert_almost_equal
from PIL.Image import Image as PILImage
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from tqdm import tqdm

from commands.predict import predict_one_job
from kiliautoml.utils.constants import MLTaskT, ModelFrameworkT, ToolT
from kiliautoml.utils.download_assets import download_project_images
from kiliautoml.utils.helpers import get_assets, get_project, kili_print
from kiliautoml.utils.memoization import clear_automl_cache
from kiliautoml.utils.type import AssetStatusT

# Priorities
Priorities = List[float]

# Normalized priorities (between [0, length of queue])
PrioritiesNormalized = List[int]


class PriorityQueue:
    """List of asset_index beginning with highest priority

    Compared to the queue in the standard library, this queue adds the method remove.
    """

    def __init__(self, priorities: Priorities):

        self.queue = list(np.argsort(priorities)[::-1].tolist())
        # example queue[0] is the index of the highest priority asset
        self.check_queue_validity()

    def pop(self):
        """Get the asset_index of the asset with highest priority"""
        return self.queue.pop(0)

    def remove(self, asset_index: int):
        """Remove asset_index from the queue"""
        self.queue.remove(asset_index)
        self.check_queue_validity()

    def append(self, asset_index: int):
        """Append asset_index after the queue."""
        self.queue.append(asset_index)
        self.check_queue_validity()

    def is_empty(self):
        return len(self.queue) == 0

    def to_prioritized_list(self) -> PrioritiesNormalized:
        priority = [0] * len(self.queue)
        for i, asset_index in enumerate(self.queue):
            # i = 0 mean top-0 priority
            priority[asset_index] = len(self.queue) - 1 - i
        return priority

    def check_queue_validity(self):
        # no duplicates
        assert len(self.queue) == len(set(self.queue))

        # only integer values
        assert all([isinstance(asset_index, int) for asset_index in self.queue])

    def __repr__(self):
        return f"Queue({self.queue}) (first element is most prioritized)"


def pop_queues(queue_a: PriorityQueue, queue_b: PriorityQueue, queue: PriorityQueue):
    """Pop the highest priority asset_index from queue_a append to queue.

    remove the asset_index from queue_b.
    """
    asset_index = queue_a.pop()
    queue.append(asset_index)
    queue_b.remove(asset_index)


def normalize_priorities(
    priorities: Priorities,
) -> PrioritiesNormalized:
    """Normalize the priorities between [Ã˜, length of queue]"""
    return PriorityQueue(priorities).to_prioritized_list()


class Prioritizer:
    def __init__(
        self,
        embeddings: np.ndarray,  # type: ignore
        predictions_probability: List[float] = [],
    ):
        """Initialize Prioritizer class.

        Args:
            embeddings: matrix of embeddings
            predictions_probability: list of probability

        Raises:
        ConnectionError: If no available port is found.
        """
        assert len(embeddings) > 0 and len(embeddings[0]) > 0
        self.embeddings = embeddings
        self.predictions_probability = predictions_probability

        if predictions_probability:
            assert len(embeddings) == len(predictions_probability)

    def _get_priorities_diversity_sampling(self) -> PrioritiesNormalized:
        """Implement diversity sampling.

        We cluster the embeddings and then we create a priority list for each cluster.
        Then we combine the priority lists.

        Returns:
            List[int]: list of priorities.
        """
        embeddings = self.embeddings

        pipe = Pipeline([("pca", PCA(n_components=10)), ("kmeans", KMeans(n_clusters=5))])

        X_clusters = pipe.fit_transform(embeddings)[:, 0]

        index_clusters = {i: np.where(X_clusters == i)[0] for i in np.unique(X_clusters)}
        index_clusters_permuted = {
            i: np.random.permutation(index_clusters[i]).tolist() for i in index_clusters
        }

        sampling = []
        for _ in range(len(embeddings)):
            cluster_id = np.random.choice(list(index_clusters_permuted.keys()))
            index = index_clusters_permuted[cluster_id].pop(0)

            if len(index_clusters_permuted[cluster_id]) == 0:
                del index_clusters_permuted[cluster_id]

            sampling.append(index)

        priorities = np.argsort(np.array(sampling))

        return priorities.tolist()

    def _get_priorities_uncertainty_sampling(self) -> PrioritiesNormalized:
        """Implement uncertainty sampling with least Confidence strategy.

        Least Confidence: difference between the most confident prediction and 100% confidence.

        Returns:
            List[int]: list of priorities.
        """
        predictions_probability = self.predictions_probability
        if not predictions_probability:
            ValueError("No predictions_probability available.")
        prio = list(-np.array(predictions_probability))

        # We normalize the priorites to be between 0 and len(list)
        queue = PriorityQueue(prio)
        return queue.to_prioritized_list()

    def _get_random_sampling_priorities(self) -> PrioritiesNormalized:
        """Implement random sampling"""
        assert len(self.embeddings) > 0
        return np.random.permutation(len(self.embeddings)).tolist()

    @staticmethod
    def combine_priorities(
        priorities_a: Priorities, priorities_b: Priorities, proba_a: float = 0.5
    ) -> PrioritiesNormalized:
        """Combine two priority lists.

        Args:
            priorities_a (List[int]): first priority list
            priorities_b (List[int]): second priority list
            proba_a (float, optional): probability of taking the first priority list.

        Warning:
            This function returns a normalized priority list.

        Returns:
            List[int]: combined priority list
        """
        assert len(priorities_a) == len(priorities_b)
        assert 0 <= proba_a <= 1

        queue_a = PriorityQueue(priorities_a)
        queue_b = PriorityQueue(priorities_b)

        queue_combined = PriorityQueue([])
        for _ in priorities_a:
            if np.random.random() < proba_a:
                pop_queues(queue_a, queue_b, queue_combined)
            else:
                pop_queues(queue_b, queue_a, queue_combined)

        return queue_combined.to_prioritized_list()

    @staticmethod
    def combine_multiple_priorities(
        priorities: List[Priorities], probas: List[float]
    ) -> PrioritiesNormalized:
        assert_almost_equal(np.sum(probas), 1)

        n_assets = len(priorities[0])
        assert max([len(x) for x in priorities]) == n_assets
        assert min([len(x) for x in priorities]) == n_assets

        assert len(probas) == len(priorities)

        if len(probas) == 1:
            return normalize_priorities(priorities[0])
        elif len(probas) == 2:
            return Prioritizer.combine_priorities(priorities[0], priorities[1], probas[0])
        else:
            argsort = np.argsort(np.array(probas))

            priorities_sorted = [priorities[i] for i in argsort]
            probas_sorted = [probas[i] for i in argsort]

            if probas_sorted[0] < 0.0001:
                # The first priority queue should be ignored
                return Prioritizer.combine_multiple_priorities(
                    priorities_sorted[1:], probas_sorted[1:]
                )
            else:
                # All priority queues should be considered
                other_probas = np.array(probas_sorted[:2])
                other_priorities = Prioritizer.combine_multiple_priorities(
                    priorities_sorted[:2], other_probas / np.sum(other_probas)
                )
                return Prioritizer.combine_priorities(
                    priorities_sorted[0],
                    other_priorities,  # type: ignore
                    probas_sorted[0],
                )

    def get_priorities(
        self, diversity_sampling: float, uncertainty_sampling: float
    ) -> PrioritiesNormalized:
        """diversity_sampling is a float between 0 and 1"""
        assert 0 <= diversity_sampling <= 1
        assert 0 <= uncertainty_sampling <= 1
        assert 0 <= uncertainty_sampling + diversity_sampling <= 1

        random_sampling = 1 - diversity_sampling - uncertainty_sampling
        kili_print(
            f"Sampling Mix of {diversity_sampling*100}% of  Diversity Sampling "
            f"and {uncertainty_sampling*100}% of Uncertainty Sampling "
            f"and {random_sampling*100}% of Random Sampling."
        )

        diversity_sampling_priorities = self._get_priorities_diversity_sampling()
        uncertainty_sampling_priorities = self._get_priorities_uncertainty_sampling()
        random_sampling_priorities = self._get_random_sampling_priorities()

        priorities = [
            diversity_sampling_priorities,
            uncertainty_sampling_priorities,
            random_sampling_priorities,
        ]

        combined_priorities = self.combine_multiple_priorities(
            priorities,  # type: ignore
            [
                diversity_sampling,
                uncertainty_sampling,
                random_sampling,
            ],
        )

        return combined_priorities


def embeddings_images(images: List[PILImage], batch_size=4) -> np.ndarray:  # type: ignore
    """Get the embeddings of the images using a generic model trained on ImageNet."""
    color_images = [im.convert("RGB") for im in images]
    img2vec = Img2Vec(cuda=torch.cuda.is_available(), model="efficientnet_b7")
    vecs = []
    for imgs in tqdm(list(chunked(color_images, batch_size))):
        _ = np.array(img2vec.get_vec(imgs))
        vecs.append(_)
    return np.concatenate(vecs, axis=0)


def embeddings_ner(
    texts: List[str],
):
    raise NotImplementedError


def embedding_text(
    texts: List[str],
):
    raise NotImplementedError


@click.command()
@click.option(
    "--api-endpoint",
    default="https://cloud.kili-technology.com/api/label/v2/graphql",
    help="Kili Endpoint",
)
@click.option("--api-key", default=os.environ.get("KILI_API_KEY"), help="Kili API Key")
@click.option("--project-id", required=True, help="Kili project ID")
@click.option(
    "--asset-status-in",
    default=["TODO", "ONGOING"],
    callback=lambda _, __, x: x.upper().split(",") if x else [],
    help=(
        "Comma separated (without space) list of Kili asset status to select "
        "among: 'TODO', 'ONGOING', 'LABELED', 'TO_REVIEW', 'REVIEWED'"
        "Example: python train.py --asset-status-in TO_REVIEW,REVIEWED "
    ),
)
@click.option(
    "--max-assets",
    default=None,
    type=int,
    help="Maximum number of assets to consider",
)
@click.option(
    "--diversity-sampling",
    default=0.3,
    type=float,
    help="Diversity sampling proportion",
)
@click.option(
    "--model-framework", default="pytorch", help="Model framework (eg. pytorch, tensorflow)"
)
@click.option(
    "--uncertainty-sampling",
    default=0.4,
    type=float,
    help="Uncertainty sampling proportion",
)
@click.option(
    "--dry-run",
    default=None,
    is_flag=True,
    help="Runs the predictions but do not save them into the Kili project",
)
@click.option(
    "--from-model",
    default=None,
    help="Runs the predictions using a specified model path",
)
@click.option(
    "--verbose",
    default=0,
    help="Verbose level",
)
@click.option(
    "--clear-dataset-cache",
    default=False,
    is_flag=True,
    help="Tells if the dataset cache must be cleared",
)
@click.option(
    "--from-project",
    default=None,
    type=str,
    help=(
        "Use a model trained of a different project to predict on project_id."
        "This is usefull if you do not want to pollute the original project with "
        "experimental predictions."
        "This argument is ignored if --from-model is used."
    ),
)
@click.option(
    "--batch-size",
    default=8,
    type=int,
    help="Maximum number of assets to consider",
)
def main(
    api_endpoint: str,
    api_key: str,
    project_id: str,
    asset_status_in: List[AssetStatusT],
    max_assets: Optional[int],
    diversity_sampling: float,
    uncertainty_sampling: float,
    dry_run: bool,
    from_model: ModelFrameworkT,
    verbose: bool,
    clear_dataset_cache: bool,
    from_project: Optional[str],
    model_name: Optional[str],
    model_repository: Optional[str],
    model_framework: str,
    batch_size: int,
):
    """
    Prioritize assets in a Kili project.

    The goal is to find a collection of assets that are most relevant to prioritize.
    We embedded the assets using a generic model, and then use a strategy that is a mixture of
    diversity sampling, uncertainty sampling, and random sampling to sorts the assets.
    """
    if uncertainty_sampling + diversity_sampling > 1:
        raise ValueError("diversity_sampling + diversity_sampling should be less than 1.")

    if max_assets and max_assets < 10:
        raise ValueError("max_assets should be greater than 10")

    kili = Kili(api_key=api_key, api_endpoint=api_endpoint)
    input_type, jobs, _ = get_project(kili, project_id)
    kili_print("Input type: ", input_type)
    kili_print("jobs: ", jobs)

    jobs_item = list(jobs.items())
    if len(jobs_item) > 1:
        raise NotImplementedError

    job_name, job = jobs_item[0]
    content_input = job.get("content", {}).get("input")
    ml_task: MLTaskT = job.get("mlTask")  # type: ignore
    tools: ToolT = job.get("tools")  # type: ignore

    if clear_dataset_cache:
        clear_automl_cache(
            command="prioritize", project_id=project_id, job_name=job_name, model_repository=None
        )

    unlabeled_assets = get_assets(
        kili,
        project_id,
        asset_status_in,
        max_assets,
    )

    # TODO: useless if uncertainty_sampling == 0
    job_predictions = predict_one_job(
        api_key=api_key,
        api_endpoint=api_endpoint,
        project_id=project_id,
        from_model=from_model,
        verbose=verbose,
        input_type=input_type,
        assets=unlabeled_assets,
        batch_size=batch_size,
        job_name=job_name,
        job=job,
        content_input=content_input,
        model_repository=model_repository,
        model_framework=model_framework,
        model_name=model_name,
        ml_task=ml_task,
        tools=tools,
        from_project=from_project,
        clear_dataset_cache=clear_dataset_cache,
    )

    if input_type == "IMAGE":
        downloaded_images = download_project_images(api_key, unlabeled_assets, output_folder=None)
        pil_images = [image.image for image in downloaded_images]
        embeddings = embeddings_images(pil_images)
        kili_print("Embeddings successfully computed with shape ", embeddings.shape)
    else:
        raise NotImplementedError

    predictions_probability = job_predictions.predictions_probability
    print("Predictions probability shape: ", predictions_probability)
    asset_ids = [asset["id"] for asset in unlabeled_assets]
    prioritizer = Prioritizer(embeddings, predictions_probability=predictions_probability)
    priorities = prioritizer.get_priorities(
        diversity_sampling=diversity_sampling, uncertainty_sampling=uncertainty_sampling
    )
    if not dry_run:
        kili.update_properties_in_assets(asset_ids=asset_ids, priorities=priorities)


if __name__ == "__main__":
    main()
